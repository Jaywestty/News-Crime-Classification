{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jaywestty/News-Crime-Classification/blob/main/Text_summarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAS3oFqUUf_F"
      },
      "source": [
        "### **NEWS TEXT SUMMARIZER PROJECT**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Project Description:**\n",
        "This project aims to automatically summarize news articles into concise, factual highlights using Hugging Face Transformers. The summarization model is based on the bart-base architecture, chosen for its strong performance on abstractive summarization while remaining lightweight enough to run within Google Colab's free-tier resource limits. The dataset, sourced from Hugging Face’s public datasets repository, contains diverse news articles for training and evaluation. The system is designed to generate short, accurate, and easily readable summaries that retain the key points of the original article, making it useful for quick news consumption."
      ],
      "metadata": {
        "id": "TshE3YFxJ0rP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUYkV0fBWQfm"
      },
      "source": [
        "#### **Install dependecies**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets evaluate rouge_score accelerate nltk -q"
      ],
      "metadata": {
        "id": "j_Dv9hJc9AIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Import required libraries**"
      ],
      "metadata": {
        "id": "erZVTiIKKr3o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBh79AK2VPDY"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer, DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "import numpy as np\n",
        "import torch\n",
        "import nltk\n",
        "import gc\n",
        "import evaluate\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Clear up memory to aid colab limit\n",
        "def clear_memory():\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "cNWZ2DgO2oow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Load Bart tokenizer and model**"
      ],
      "metadata": {
        "id": "lWXJ608OK4IS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"facebook/bart-base\"\n",
        "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
        "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "print(f\"Model loaded! Parameters: {model.num_parameters():,}\")\n",
        "clear_memory()"
      ],
      "metadata": {
        "id": "tAFicTXP23sx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Load Dataset**"
      ],
      "metadata": {
        "id": "BRDuY5nTLFQF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading CNN-DailyMail dataset...\")\n",
        "dataset = load_dataset('cnn_dailymail', '3.0.0')\n",
        "\n",
        "print(\"Sample article:\\n\", dataset['train'][0]['article'][:200])\n",
        "print(\"\\nSample summary:\\n\", dataset['train'][0]['highlights'])\n",
        "\n",
        "# Reduce dataset for Colab constraints\n",
        "train_dataset = dataset['train'].select(range(8000))  # Slightly smaller for BART\n",
        "val_dataset = dataset['validation'].select(range(800))\n",
        "test_dataset = dataset['test'].select(range(800))\n",
        "\n",
        "print(f\"Dataset sizes - Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")"
      ],
      "metadata": {
        "id": "kZZGU0BQ3fek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **BART-specific preprocessing**"
      ],
      "metadata": {
        "id": "SEOQagRqLSJw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUeOer1YBJhl"
      },
      "outputs": [],
      "source": [
        "max_input_length = 1024  # BART can handle longer inputs\n",
        "max_target_length = 142  # CNN-DM standard summary length\n",
        "\n",
        "def preprocess(example):\n",
        "    model_inputs = tokenizer(\n",
        "        example['article'],\n",
        "        max_length=max_input_length,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\"  # Changed from True\n",
        "    )\n",
        "\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(\n",
        "            example['highlights'],\n",
        "            max_length=max_target_length,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\"\n",
        "        )\n",
        "\n",
        "    # Replace pad token id with -100 for label loss masking\n",
        "    labels_ids = labels[\"input_ids\"]\n",
        "    labels_ids = [\n",
        "        [(token if token != tokenizer.pad_token_id else -100) for token in label]\n",
        "        for label in labels_ids\n",
        "    ]\n",
        "    model_inputs[\"labels\"] = labels_ids\n",
        "    return model_inputs\n",
        "\n",
        "\n",
        "print(\"Preprocessing datasets...\")\n",
        "train_tokenized = train_dataset.map(preprocess, batched=True, remove_columns=train_dataset.column_names)\n",
        "val_tokenized = val_dataset.map(preprocess, batched=True, remove_columns=val_dataset.column_names)\n",
        "\n",
        "clear_memory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vJYmqxxQ9Ud"
      },
      "outputs": [],
      "source": [
        "print(train_tokenized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tiQnskkmjJSN"
      },
      "outputs": [],
      "source": [
        "print(\"Pad token ID:\", tokenizer.pad_token_id)\n",
        "print(\"Vocab size:\", tokenizer.vocab_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Data Collator (Dynamic Padding)**"
      ],
      "metadata": {
        "id": "DsjP6acfNpSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
      ],
      "metadata": {
        "id": "Bxmchea__WPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Load ROUGE for Evaluation**"
      ],
      "metadata": {
        "id": "-Z1BRQQ8Nzfk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    labels = [[(token if token != -100 else tokenizer.pad_token_id) for token in label] for label in labels]\n",
        "    labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    result = rouge.compute(predictions=predictions, references=labels, use_stemmer=True)\n",
        "    return {k: round(v * 100, 2) for k, v in result.items()}"
      ],
      "metadata": {
        "id": "oVTPT9Lh_beX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Training Arguments**"
      ],
      "metadata": {
        "id": "gHZd5CIBN6HB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./bart-news-summarizer\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=2,  # Small for Colab memory\n",
        "    per_device_eval_batch_size=2,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=2,\n",
        "    num_train_epochs=1,\n",
        "    predict_with_generate=True,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    logging_dir='./logs',\n",
        ")\n"
      ],
      "metadata": {
        "id": "W9yAk4dW_qp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Trainer Setup**"
      ],
      "metadata": {
        "id": "u0ggKib5OBI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_tokenized,\n",
        "    eval_dataset=val_tokenized,\n",
        "    processing_class=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "-RCQM2CB_5Qx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Train the Model**"
      ],
      "metadata": {
        "id": "L2g5O5hsOHbg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKB9XNNHWe87"
      },
      "outputs": [],
      "source": [
        "clear_memory()\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Evaluatin of Model**"
      ],
      "metadata": {
        "id": "3BlZ7H75OWiU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = trainer.evaluate()\n",
        "print(metrics)"
      ],
      "metadata": {
        "id": "JsjT_YpGR89H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Test the Model on an article**"
      ],
      "metadata": {
        "id": "to7l3Y1dOdtq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_and_merge_article(article):\n",
        "    # Step 1: Clean article text\n",
        "    article = re.sub(r\"\\s+\", \" \", article.strip())  # collapse spaces & newlines\n",
        "    article = article.replace(\" ,\", \",\").replace(\" .\", \".\")  # fix space before punctuation\n",
        "\n",
        "    # Step 2: Summarize using your model\n",
        "    inputs = tokenizer(article, return_tensors=\"pt\", max_length=1024, truncation=True).to(model.device)\n",
        "    summary_ids = model.generate(**inputs, max_length=142, min_length=56, length_penalty=2.0, num_beams=4)\n",
        "    raw_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    # Step 3: Merge summary into one sentence\n",
        "    summary = re.sub(r'\\s+', ' ', raw_summary.strip())\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+(?=[A-Z])', summary)\n",
        "    sentences = [s.strip(\" .\") for s in sentences if s.strip()]\n",
        "\n",
        "    if not sentences:\n",
        "        return \"\"\n",
        "    if len(sentences) == 1:\n",
        "        return sentences[0] + \".\"\n",
        "\n",
        "    merged = \", \".join(sentences[:-1]) + \" and \" + sentences[-1]\n",
        "    return merged.strip() + \".\"\n"
      ],
      "metadata": {
        "id": "I1WbJxBlBBUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article = \"\"\"\n",
        "          President Bola Tinubu has paid tribute to a former Minister of Agriculture and ex-Peoples Democratic Party’s National Chairman, Chief Audu Ogbeh, describing him as a patriot who believed deeply in the nation’s potential.The family of the former minister announced his death in a statement earlier on Saturday.In a statement on Saturday by his Special Adviser on Information and Strategy, Bayo Onanuga, the President said the late politician was “always ready with facts and figures to support his propositions” and “a man of strong convictions who spoke the truth as he saw it.”The President added that Ogbeh’s contributions to Nigeria’s agricultural development, political stability, and democratic growth “will remain indelible in the country’s history.”Tinubu extended condolences to the government and people of Benue State, as well as Ogbeh’s family, friends, and associates, praying “the Almighty God will receive his soul and comfort his family at this difficult time.”\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "0ZO5O1EUFfYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean = clean_and_merge_article(article)\n",
        "print(clean)"
      ],
      "metadata": {
        "id": "zm9nv0tLF5p5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x6HkLVF-HlBK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMl9EvbwkFqtuyikBAdCEQw",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}